# -*- coding: utf-8 -*-
"""Student Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1laupQ8kv-0Ld-G694FqDUMjiFHzNrryQ
"""



import pandas as pd
import re
import nltk

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

nltk.download('punkt_tab')
nltk.download('punkt')

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer


file_path = "./Sentiment Data Collection Sheet.xlsx"

all_sheets = pd.read_excel(file_path, sheet_name=None)

merged_data = pd.concat(all_sheets.values(), ignore_index=True)

print(merged_data.head())
merged_data.info()

# Data preprocessing

# Drop rows with any empty feedback
# merged_data = merged_data.dropna()

# Remove any extraneous whitespace in the columns
merged_data.columns = merged_data.columns.str.strip()

# Here, we'll fill NaN with an empty string to avoid issues during concatenation.
merged_data.fillna('', inplace=True)

# display(merged_data)
otput = merged_data.to_excel('output.xlsx', index=False)
merged_data.info()

merged_data['Combined Feedback'] = merged_data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

# display(merged_data)

# Define preprocessing function

def preprocess_text(text):

    # convert to lowercase
    text = re.sub(r'\[REDACTED\]', '', text)
    text = text.lower()

    # remove the term "[REDACTED]"

    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)

    # Optional: Remove extra whitespace if necessary
    text = re.sub(r'\s+', ' ', text).strip()


    # Tokenize
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.lower() not in stop_words]
    return ' '.join(tokens)

# Applying the processing function to the reviews_df's 'Text' column and storing in a new column named preprocessed_review

columns_to_process = merged_data.columns.difference(['Combined Feedback'])

# Apply preprocess_text to all columns except "Combined Feedback"

merged_data[columns_to_process] = merged_data[columns_to_process].map(preprocess_text)

# Now apply preprocess_text to the Combined Feedback column only

merged_data['Combined Feedback'] = merged_data['Combined Feedback'].apply(preprocess_text)
# display(merged_data)



# Reset index to create an 'Id' column in merged_data

merged_data = merged_data.reset_index().rename(columns={'index': 'Id'})

# Using SentimentIntensityAnalyzer module to get the neg, neu, pos and compound scores

sentiment = SentimentIntensityAnalyzer()

res = {}
for i, row in merged_data.iterrows():
    text = row['Combined Feedback']
    res[i] = sentiment.polarity_scores(text)

vaders = pd.DataFrame(res).T
vaders = vaders.reset_index().rename(columns={'index': 'Id'})

# Merging the vaders daaset with the original reviews_df

vaders = vaders.merge(merged_data, on='Id', how='left')
# display(vaders.head())

import matplotlib.pyplot as plt

# Plot each sentiment score in a separate subplot
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
vaders['neg'].hist(bins=20, ax=axes[0, 0], color='skyblue', edgecolor='white')
axes[0, 0].set_title('Negative Sentiment')
axes[0, 0].set_xlabel('Negative Score')
axes[0, 0].set_ylabel('Frequency')
vaders['neu'].hist(bins=20, ax=axes[0, 1], color='lightgreen', edgecolor='white')
axes[0, 1].set_title('Neutral Sentiment')
axes[0, 1].set_xlabel('Neutral Score')
axes[0, 1].set_ylabel('Frequency')
vaders['pos'].hist(bins=20, ax=axes[1, 0], color='salmon', edgecolor='white')
axes[1, 0].set_title('Positive Sentiment')
axes[1, 0].set_xlabel('Positive Score')
axes[1, 0].set_ylabel('Frequency')
vaders['compound'].hist(bins=20, ax=axes[1, 1], color='lightcoral', edgecolor='white')
axes[1, 1].set_title('Compound Sentiment')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_xlabel('Compound Score')
plt.tight_layout()
plt.show()

print(vaders[['neg', 'neu', 'pos', 'compound']].describe())
# display(vaders)

# Word Clouds for Positive and Negative Feedback
from wordcloud import WordCloud

# Positive Feedback
positive_text = ' '.join(vaders[vaders['compound'] > 0.05]['Combined Feedback'])
positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
plt.figure(figsize=(10, 5))
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Positive Feedback Word Cloud').set_fontfamily('Cambria')
plt.show()

# Negative Feedback
negative_text = ' '.join(vaders[vaders['compound'] < -0.05]['Combined Feedback'])
negative_wordcloud = WordCloud(width=800, height=400, background_color='black').generate(negative_text)
plt.figure(figsize=(10, 5))
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Negative Feedback Word Cloud')
plt.show()

# Sentiment distribution by question
# Step 1: Concatenate all feedback entries for each question column

merged_feedback = {
    "Performance_Improvement": ' '.join(vaders["I could have done the following to improve my performance in this course:"].dropna()),
    "Course_Strengths": ' '.join(vaders["Please identify what you consider to be the strengths of this course."].dropna()),
    "Areas_to_Improve": ' '.join(vaders["Please identify areas where you think this course could be improved."].dropna()),
    "Advice_to_Students": ' '.join(vaders["What advice would you give to another student who is considering taking this course?"].dropna()),
    "Instructor_Effectiveness": ' '.join(vaders["What suggestions do you have to improve the instructor's effectiveness?"].dropna())
}
#display(merged_feedback)

# Apply vader Sentiment analyzer

analyzer = SentimentIntensityAnalyzer()
sentiment_scores = {question: analyzer.polarity_scores(feedback) for question, feedback in merged_feedback.items()}
# display(sentiment_scores)

# Convert the results to a dataframe for easier visualization
# Transpose to have questions as rows

sentiment_df = pd.DataFrame(sentiment_scores).T
# display(sentiment_df)

sentiment_df.columns = ['Negative', 'Neutral', 'Positive', 'Compound']
# display(sentiment_df)

# Plot sentiment distribution by question

fig, ax = plt.subplots(2, 2, figsize=(12, 8))
sentiment_types = ['Negative', 'Neutral', 'Positive', 'Compound']
colors = ['skyblue', 'lightgreen', 'salmon', 'lightcoral']

for i, question in enumerate(sentiment_types):
    row, col = i // 2, i % 2
    sentiment_df[question].plot(kind='bar', ax=ax[row][col], color=colors[i])
    ax[row][col].set_title(f'{question} Sentiment by Question')
    ax[row][col].set_xlabel('Question')
    ax[row][col].set_ylabel(f'{question} Score')
    ax[row][col].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from gensim import corpora
from gensim.models import LdaModel

# Convert tokenized text into a format suitable for Gensim
merged_data['Combined Feedback'] = merged_data['Combined Feedback'].apply(lambda x: x if isinstance(x, list) else str(x).split())
dictionary = corpora.Dictionary(merged_data['Combined Feedback'])

corpus = [dictionary.doc2bow(text) for text in merged_data['Combined Feedback']]

# Train LDA model using Gensim
num_topics = 5  # You can adjust this based on coherence score later
lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)

# Display top words per topic
topics = lda_model.print_topics(num_words=10)
for topic in topics:
    print(topic)

# Visualize LDA topics using pyLDAvis
lda_visualization = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(lda_visualization)


